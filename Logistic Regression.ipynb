{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def list2Arr(lis, wordPos):\n",
    "    wcnt = np.zeros(len(wordPos))\n",
    "    for word in lis:\n",
    "        try:\n",
    "            wcnt[wordPos[word]] += 1\n",
    "        except:\n",
    "            continue\n",
    "    return wcnt\n",
    "\n",
    "def tf_idf(url):\n",
    "    # Data Loading\n",
    "    corpus = sc.textFile(url) ## url\n",
    "    # each entry in validLines will be a line from the text file\n",
    "    validLines = corpus.filter(lambda x: 'id' in x)\n",
    "    # now we transform it into a bunch of (docID, text) pairs\n",
    "    keyAndText = validLines.map(lambda x :(x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\">') + 2:]))\n",
    "    # now we split the text in each (docID, text) pair into a list of words\n",
    "    # after this, we have a data set with (docID, [\"word1\", \"word2\", \"word3\", ...])\n",
    "    # we have a bit of fancy regular expression stuff here to make sure that we do not\n",
    "    # die on some of the documents\n",
    "    regex = re.compile('[^a-zA-Z]')\n",
    "    keyAndListOfWords = keyAndText.map(lambda x : (str(x[0]), regex.sub(' ', x[1]).lower().split()))\n",
    "    # now get the top 20,000 words... first change (docID, [\"word1\", \"word2\", \"word3\", ...])\n",
    "    # to (\"word1\", 1) (\"word2\", 1)...\n",
    "    allWords = keyAndListOfWords.flatMap(lambda x:((j, 1) for j in x[1]))\n",
    "    # now, count all of the words, giving us (\"word1\", 1433), (\"word2\", 3423423), etc.\n",
    "    allCounts = allWords.reduceByKey(lambda a, b: a + b)\n",
    "    # and get the top 20,000 words in a local array\n",
    "    # each entry is a (\"word1\", count) pair\n",
    "    topWords = allCounts.top(20000, lambda x : x[1])\n",
    "    Dict_wPos = {}\n",
    "    for i in range(len(topWords)):\n",
    "        Dict_wPos[topWords[i][0]] = i\n",
    "    T1res = keyAndListOfWords.map(lambda x:(x[0], list2Arr(x[1], Dict_wPos)))\n",
    "    # TF\n",
    "    tf = T1res.map(lambda x: (x[0], x[1] / x[1].sum()))\n",
    "    # IDF\n",
    "    nDoc = corpus.count()\n",
    "    p = T1res.map(lambda x: (x[0], np.clip(x[1], 0, 1)))\n",
    "    q = p.map(lambda x: (\"nDoc\", x[1])) \n",
    "    nWinDoc = q.reduceByKey(lambda a, b: a + b)\n",
    "    idf = np.log(nDoc / nWinDoc.lookup(\"nDoc\")[0])\n",
    "    # TF-IDF\n",
    "    tf_idf = tf.map(lambda x: (x[0], x[1] * idf))\n",
    "    # Normalization\n",
    "    mean = tf_idf.values().sum() / tf_idf.count()\n",
    "    std_dev = np.sqrt(tf_idf.map(lambda x: np.square(x[1] - mean)).reduce(lambda a, b: a+b) / float(tf_idf.count()))\n",
    "    return tf_idf, topWords, mean, std_dev\n",
    "\n",
    "def normalize(mean, std_dev, tf_idf):\n",
    "    tf_idf_norm = tf_idf.map(lambda x: (x[0], np.nan_to_num((x[1] - mean)/std_dev))).cache().sortByKey()\n",
    "    return tf_idf_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_train1, tw_train, mean, std = tf_idf(\"s3://chrisjermainebucket/comp330_A5/TestingDataOneLinePerDoc.txt\")\n",
    "tf_idf_train1 = normalize(mean, std, tf_idf_train1)\n",
    "tf_idf_train1.cache()\n",
    "\n",
    "## sort accoding to count\n",
    "topWords_cw = sorted(tw_train, key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "topw = sc.parallelize(range(20000))\n",
    "w_dict = topw.map(lambda x: (topWords_cw[x][0], x))\n",
    "w_dict.cache().sortByKey()\n",
    "\n",
    "#Prints 347\n",
    "w_dict.lookup(\"applicant\")[0]\n",
    "\n",
    "#Prints 2\n",
    "w_dict.lookup(\"and\")[0]\n",
    "\n",
    "#Prints 504\n",
    "w_dict.lookup(\"attack\")[0]\n",
    "\n",
    "#Prints 3014\n",
    "w_dict.lookup(\"protein\")[0]\n",
    "\n",
    "#Prints 612\n",
    "w_dict.lookup(\"car\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original: $LLH = \\sum_i[(y_i\\theta^Tx_i - log(1 + e^{\\theta^Tx_i})]$,  where: $y_i \\in \\{0,1\\}, x \\in R^{20000}, \\theta \\in R^{20000} $\n",
    "\n",
    "After regularization: $LLH = \\sum_i[(y_i\\theta^Tx_i - log(1 + e^{\\theta^Tx_i})] + \\color{blue}{\\beta||\\theta||_2^2}$\n",
    "\n",
    "Thus:  $\\frac{\\partial LLH}{\\partial \\theta} = \\sum_i[(y_ix_i - \\frac{e^{\\theta^Tx_i}}{1 + e^{\\theta^Tx_i}}x_i] + 2\\beta||\\theta||_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate gradient for each row(document)\n",
    "def grad_doc(x,r,beta):\n",
    "    if \"AU\" in str(x[0]):\n",
    "        y_i = 1\n",
    "    else:\n",
    "        y_i = 0\n",
    "    net_i = r.dot(x[1])\n",
    "    g = np.vectorize(lambda x: -x*y_i + x*(np.exp(net_i)/(1 + np.exp(net_i))))\n",
    "    return g(x[1]) + 2*beta*r\n",
    " \n",
    "    \n",
    "# calculate negative llh for each row(document)    \n",
    "def nllh(x,r):\n",
    "    if \"AU\" in str(x[0]):\n",
    "        y_i = 1\n",
    "    else:\n",
    "        y_i = 0\n",
    "    net_i = r.dot(x[1])\n",
    "    return -y_i*net_i+np.log(1+np.exp(net_i))\n",
    "\n",
    "\n",
    "# gradient descent main function\n",
    "def gd(init_r, tf_idf, bet = 0.0001):\n",
    "    num_docs = tf_idf.count()\n",
    "    r = init_r\n",
    "    delta = 1\n",
    "    lr = 1\n",
    "    loss_now = tf_idf.map(lambda x:nllh(x,r)).reduce(lambda a,b: a+b) + bet*np.linalg.norm(r)\n",
    "    num_epoch = 0\n",
    "    while delta>0.0001:\n",
    "        num_epoch += 1\n",
    "        grad = tf_idf.map(lambda x:grad_doc(x,r,beta = bet)).reduce(lambda a,b: a+b)/num_docs\n",
    "        r -= lr*grad\n",
    "        loss_next = tf_idf.map(lambda x: nllh(x,r)).reduce(lambda a,b: a+b) + bet*np.linalg.norm(r)**2\n",
    "        delta = abs(loss_next - loss_now)\n",
    "        print(\"at epoch:\", num_epoch, \"the negative log likelihood is:\", loss_next)\n",
    "        if (loss_next > loss_now):\n",
    "            lr = lr / 2\n",
    "        else:\n",
    "            lr = lr*1.1\n",
    "        loss_now = loss_next\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using small data set to pretrain model\n",
    "r_init = np.random.randn(20000)/10\n",
    "r_pre_trained = gd(r_init, tf_idf_train1)\n",
    "\n",
    "# using large training set to retrain model\n",
    "tf_idf_train2, _, _a, _b = tf_idf(\"s3://chrisjermainebucket/comp330_A5/SmallTrainingDataOneLinePerDoc.txt\")\n",
    "tf_idf_train2 = normalize(mean, std, tf_idf_train2)\n",
    "tf_idf_train2.cache()\n",
    "r_trained = gd(r_pre_trained, tf_idf_train2)\n",
    "\n",
    "top_50 = r_trained.argsort()[-50:][::-1]\n",
    "w_dict_reverse = w_dict.map(lambda x: (x[1],x[0])).cache().sortByKey()\n",
    "\n",
    "for idx in top_50:\n",
    "    print(w_dict_reverse.lookup(idx),\"'s parameter is:\", r_trained[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_evaluate(test, r, cut = 0):\n",
    "    y_true_raw = test.map(lambda x: ([1] if \"AU\" in str(x[0]) else [0], x[1]))\n",
    "    y_true_pred = y_true_raw.map(lambda x: (x[0], [1] if r.dot(x[1]) > cut else [0]))\n",
    "    res = np.array(y_true_pred.collect())\n",
    "    \n",
    "    TP = 0 # True positive\n",
    "    TN = 0 # True negative\n",
    "    FP = 0 # False positive\n",
    "    FN = 0 # False negative\n",
    "    \n",
    "    FP_indx = []\n",
    "    \n",
    "    for index in range(res.shape[0]):\n",
    "        if ((res[index,0] == 1) and (res[index,1] == 1)):\n",
    "            TP += 1\n",
    "        elif ((res[index,0] == 0) and (res[index,1] == 0)):\n",
    "            TN += 1\n",
    "        elif ((res[index,0] == 0) and (res[index,1] == 1)):\n",
    "            FP += 1\n",
    "            FP_indx.append(index)\n",
    "        elif ((res[index,0] == 1) and (res[index,1] == 0)):           \n",
    "            FN += 1\n",
    "    print(TP,FP,TN,FN)        \n",
    "    accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "    recall = TP / (TP + FN)\n",
    "    precision = TP/ (TP + FP)\n",
    "    F1 = 2 * precision * recall / (precision + recall)\n",
    "    print(\"test accuracy:\",round(accuracy, 5))\n",
    "    print(\"recall:\", round(recall,5))\n",
    "    print(\"precision\", round(precision,5))\n",
    "    print(\"F1 score\", round(F1,5))\n",
    "    \n",
    "    if len(FP_indx) > 0:\n",
    "        print('There are', len(FP_indx), \"false positive cases.\")\n",
    "        for idx in FP_indx:\n",
    "            FP_webpage = test.take(idx)[idx-1]\n",
    "            print(FP_webpage[0])\n",
    "    return accuracy, recall, precision, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_test, _, _a, _b = tf_idf(\"s3://chrisjermainebucket/comp330_A5/TestingDataOneLinePerDoc.txt\")\n",
    "tf_idf_test = normalize(mean, std, tf_idf_test)\n",
    "acc, rcl, prc, f1 = predict_evaluate(tf_idf_test, r_trained, cut = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
